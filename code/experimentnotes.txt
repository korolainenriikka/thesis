vaikuttaa olevan että alaleuka on paljon yleisempi kuin yläleuka

dataset creation
w regex -> c issue
with this segmentation many of the c's we have in the dataset are not actually tooth marking c's -> many c's were missing up/low, but cs are few so the extra cs come definitely in handy
but then undecipherable c's in up-low are many so the undecipherables might learn to be associated with the letter c not missing the relevant information...
frequent undecipherables issue: maybe in some catalogue could be that just upper is marked and other is lower? i dont know and i dont have the context info nor does the model
labeling mistakes: i corrected some on a third doublecheck round. would be interesting to know how many errors are left in datasets, people seem to just assume that labels are always correct.

up/low undecipherable-kysymys
syyn sille miks en poista undecipherableja training datasta vaan sisällytän ne (on hyödyllistä saada inferencessä tieto että tämä ei ole pääteltävissä eli on hyvä että se on erillinen luokka)
undecipherable is very few in the data set. maybe would be better after all to omit undecipherables and set a class confusion threshold? but then again we have to mark as up or low due to notation. so force as up/low every time but mark row with low confidence when there is uncertainty so no ternary label sittenki. (because otherwise class balance would be very screwed)

results/discussion
tää on myös et jos hand labeler tekee virheitä koska ihan mahollista kun näin paljon kuvia. kiinnostavaa miettiä itseläbelöidyssä mistä tietää oliko virheitä
siis oikeesti oikeessa datassa on oikee juttu onko label erroreita, luulen et on sinisilmästä olettaa että ei ole....
mun jutun ongelma on sit kans mun pipelinen, että jos ois jotain kontekstisidonnaista kuten rigt mandible with m1-3, c, tiedetään että on alaleuka. sanoittain käsittely hukkaa nää. vois esim prosessoida sanat ja jälkiprosessoida kontekstitiedolla tai jotain, en tiedä. Monimutkainen kymysys!
how good the optimization was affected the results a lot... so from previous work maybe it is not that good of a idea to too blindly trust the which one is the best model, why dont i just do some exhaustive search
at least alexnet might be bad with very low contrast
azure was good there so maybe use azure output when the tooth is valid and confidence is high
i could somehow encode the knowledge that info on up low jaw is in the index number usually but that would be feature engineering, but kinda prior info encoding, idk

dataset size & test error granularity

hyperparameters
hparams that worked great w alexnet dont work that great w efficientnet... maybe i should optimize lr a bit for all models? or use adam?
mini hyperparameter tuning w alexnet. lr 0.001 momentum .9 lr scheduling every 7 epochs divide lr by 10 got 10% better acc and 40 epochs less
but then efficientnet is crap
-----> i dont know if i can compare anything because optimizing is so random and hard that you can never really know whether the end result is because of the runs were optimized sometimes better sometimes worse of whether the other architecture is actually better
this is very inexact science, so this worked approach, no compare and tell why's, actually probably makes sense.

data balanssiongelmia
i:tä ei oo tarpeeksi
indekseitä ei oo ylipäätäänsä tarpeeksi.
eli trainaa kaks mallia
123 malli
1234 malli
jaa kuva horisontaalisesti puoliksi, se on inputti
p:t menee 1234:lle m ja i menee 123:lle

azure vai spesialistimalli? eli tehdäänkö mpi & 1234 mallit vai ei
(mpi ois voinu korjaa helpolla heuristiikalla H->M, T->I training data labeloinnin perusteella 1 sample ois menny väärin)
tehdään, koska
- azure tekee paljon virheitä (digit hämmentyy viivasta, esim. 2->5
	- figure tästä ja resultsiin, miksi spesialisti digit recognizer tarvitaan
- enemmän prioreita esim. azuren virheistä, enemmän vääriä positiivisia (katalogissa onkin jossain vaikka t3)
- omilla luokittelijoilla bounding box detektori voidaan vaihtaa
	azure detektori teki paljon virheitä
		esim alaraja cutoff failaa usein, yläraja vähemmän usein
- saadaan oikeempi confidence score kun luokat on rajoitettu oikein

comparint models
hard, hard to know if it is because of model or because of how good an optimum was reached, no objective metric of "how good optimum"

train val test split
up low: # train n 550, val n 100, test n 50 --> samples per class = 225. this is to have ok granularity of error but still have training samples remaining.

vit 2nd best.

on the models
confidence scores -> softmax was added to end of network (usually not put since softmax preserves ordering and classifiers just take the most likely class -> unnecessary operation)

no freezing actually
so this shows that it might be that initial examples are important, so generalist images to start is good
	although you cannot really tell, because we cannot test training only on fossil images as much as they trained on imagenet because the dataset is too small for that
and cpu rules, pretraining means  that we essentially just trained much more
	just throw data and compute at the problem and you are better off solving it
train the last layer (inspo from \cite{tibetan_ocr}, test hypothesis of reusing feature extractor)
	-> seems false, training the whole network works better. a strong result, happened with every model

base task
imagenet best: 100%, mnist best: 89%. but mnist experimentation was much less exhaustive as old work did only imagenet so take mnist result with caution

probs my main results
don't freeze
--> pick your priors, set them and throw data and compute at the problem as much as you can.
the results align with other ocr studies.

experiments
vit alexnet included: vit to test transformers alexnet to test it bc it was popular
epochs to 20 because training converged in all cases before that (CHECK)
freezing: train last 1,5,10 layers and no freezing. this instead of 123 because many models had 10-30 layers so percentage wise this is more representative

TO CHECK
convergence of training in 20 epochs every time?
sentitivity to hyperparameters?
sensitivity to random seed?
difference of train/val accuracies? -> overfitting?, find largest difference
best model test error

plots
taulukko tuloksista
fitting of best model loss plotti
freeze less -> better model for all models (line chart for accuracy vs. frozen layers

vanha
start of section: reiterate what was experimented with

up/low was a harder problem than MPI. probably because uplow notation was more versatile, but interesting because uplow had less classes

what else? find from literature!

augmentation to balance classes resulted in a 10\% increase in tooth type classification accuracy 
using a simple translation, with mnist base model. translation also makes sense since that is the main variable changing 
between images, zoom or rotation changing is more rare as these are scans.

initial phase experiments: mnist transfer to mpi since it was thought to be a similar problem. 
however like noted by \cite{alexnet}, real world image classification is much more complex than digit classification 
as the benchmark images are ideal cases with eg perfectly even background. Therefore later phases 
experimented with transfer from imagenet classifiers since that is what most of papers in related work also did.
also \cite{8goelGujarati2023} had the same result (that imagenet is better than mnist even though mnist data looks more like character images)

start with most common ie cnn, most common transfer method ie freeze all but last layer 

test both mnist and imagenet transfer 
best acc with mnist base: 89.

alexnet without any hyperparameter tuning with imagenet weights: immediately 100 percent on mpi,
upperlower surprisingly harder, 95 percent

my results are worse than other studies, why? maybe because of small data (check number of samples per class)
otherwise it seems this nn building has room for improvement
