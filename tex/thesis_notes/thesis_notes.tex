\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{subcaption}
\usepackage[style=numeric,bibstyle=numeric,backend=biber,natbib=true,maxbibnames=99,giveninits=true,uniquename=init]{biblatex}
\usepackage[utf8]{inputenc}

\addbibresource{../bibliography.bib}
\title{Thesis notes}

\begin{document}

\tableofcontents

\section{Abstract}

Keywords: Optical character recognition, Few-shot transfer learning, Vision transformers, Paleontological databases

\section{Introduction}

% outline of an introduction
% introduce the broad research area, why this is interesting
% explanation of the specific problem
% brief review of standard solutions to this (or related) problem(s) and their limitation in this case (incl. reference key papers)
% outline of the new solution
% how the solution was evaluated, what were the results of this evaluations

% relevance for other work: why was this specific problem? how can this be concretely used?
relevance of this work: any field that does ocr on unconventional characters, or constrained vocabulary.
direct relevance to museums digitizing fossil data specifically, but could be any old handwriting.

% organization
The rest of this thesis is organized as follows.


\section{Deep Neural Networks for Optical Character Recognition}

% introduction to chapter
% point of this section: present relevant deep learning theory
% running example: reading characters from images (optical character recognition, OCR)

separate problems: character classification (easy, kNN, SVM), reading variable-length text (harder) \cite{ocr_survey}
introduce the problem of ocr, example: fossil catalogue

%miscellaneous keywords to explain (maybe somewhere) from \cite{li2021trocr}
%- knowledge distillation
%- model compression
%- image patches

\subsection{Deep Neural Networks: basics}

- training/validation/test data sets
- neurons and activation functions. maybe examples of activation functions: relu, sigmoid, softmax
- feed forward, backpropagation
- weight initialization

\subsection{Training neural networks}

- learning rate
- batch size

\subsubsection{Loss functions}

Loss function is a function from model predictions and ground truth labels that describes with a single 
scalar value how good the match was, low number describing a good match \cite{princebook}.
These functions are constructed to be equivalent with maximum likelihood solution, think the model would 
output a conditional distribution of outputs, p(y|x).
each ground truth label in the training set should have a high probability in this distribution. Product of all 
these probabilities is called likelihood. Find parameters that maximize the likelihood of the training data set.
Loss functions are derived so that parameters bringing loss to zero is equivalent to the parameters with maximum likelihood.
Derivations are out of scope.

- cross-entropy loss 
kullback-leibler divergence of correct conditional probability and conditional probability parametrized by current model parameters.
(show formula, 5.27), correct is not dependent on parameters so is omitted. show 5.29, what is left from that 
(until here from \cite{princebook})

then: how cross-entropy loss is computed
% from https://machinelearningmastery.com/cross-entropy-for-machine-learning/, find a proper reference
used for classification problems. eg. is this letter in this image an 'a' or a 'b'.
correct probabilities eg .1 and .9 for a or b. model says .2 and .8. discretisized cross entropy computes 
it as .2*log.1+.8*log.9, log in base 2.

word detection models: have a predefined vocabulary, layer for probability of each word.
loss is cross entropy for these probabilities compared to target probability distribution, where correct word has probability 1 and 
all other have probability 0

- CTC loss

\subsubsection{Evaluating model performance}

- performance metrics
	- precision
	- recall 
	- f1
    - cer (character error rate)

\subsection{Architectures}

\subsubsection{Convolutional layers}

convolution (cross-correlation)
max pooling / average pooling operations

\subsubsection{Transformers}

- self-attention
- multi-head self-attention
- tokenizing and the cls token

\subsubsection{Autoencoders}

- encoder/decoder

sequence to sequence \cite{sutskever2014sequence}

\subsection{Techniques and heuristics for improving performance}

miscellaneous points, like

- data augmentation

\subsection{Transfer learning}

basics: what it is 
initialize weights to those that suit a related task, it is assumed that the starting point is already very good
catastrophic forgetting = forgetting the previously learned after finetuning

\subsubsection{Foundation models}

- generalist models
	- large unsupervised training data sets

\section{Fundamentals on paleoecology}

Nature is highly complicated -> models, approximate models and assumptions enable drawing conclusions from
known distributions of species. 

each assumption / model can be questioned but they hold as a rule. only 
models briefly presented here, all statements here can be questioned to some extent 

idea: what fossil/dental data can be used for. how fossil/dental data is used

to highlight why accurate, fine-resolution (ie specific) and large magnitude of 
fossil, esp dental fossil data is genuinely useful.

chapter overview: review ecology and assumptions the analysis is based on. 
then, short overview of main techniques for paleoenvironmental reconstruction, the main 
application area of fossil data. last, mammal teeth row is presented to introduce terminology 
present in the data.

\subsection{Basics on ecology}

basic laws: theory that the data analysis relies on

Tolerances and niches (fundamental + realized): basis for environmental reconstruction \cite{Faith_Lyman_2019} ch 2

tolerance = range of an environmental variable that is hospitable for the species, 
eg. imaginary small mammal (come up with some imaginary name) can live when avg temperature is +10-+15.
Niche = set of tolerances the species has. fundamental niche = possible environments for the species, 
realized niche = where it lives. center of tolerances is better than the edge (ch2)

main assumption uniformitarism (the fact that tolerances constraint things has not changed)
niche conservatism: Assume that nearest living relative has same tolerances now -> get past environment (ch3, lyman 2017),

this presents a mapping from taxa to environmental variables -> basis of analysis 
of past environments.

modern alternative to this: transfer functions: mappings from taxa data to enviroment 
learned using machine learning / statistical models (ch9, birks 1995)

benefit instead of tolerances/niches: they have subjective interpretation problems (book ch 9)

esp. teeth: dental ecometrics = inference of transfer functions given dental data (ch9 liu et al 2012)

next, turn to how to solve the problem of information to environment given the 
taxomic information to environmetal indicators mapping

\subsection{Paleoenvironmental reconstruction}

why: get information of what is to come with climate change (faith ch2)

definition of the problem: whan ancient habitats were like and what changes they 
underwent at which times (ch2)

overview main tehcniques: presence/absence, abundance, taxon free, diversity based,
size clines

presence absence (ch5):
dataset is list of taxa that are present. absence is also indicator 
but worse since might be that the species just was not preserved / found. two approaches: 
fix location or fix set of species.
fix species: find where this set of species lives now (climate maps \& areas of sympatry), this indicates 
that historical locality had this climate. place-fixing: analyze how which species show up in this place
 changes over time, reduce species showing up data to lower dimension
(like pca), eg how many warm-climate vs cold-climate species show up, this gives ideas on changes in climate over time

abundance 
get relative NISP (number of species in sample): percent of species in sample is of this taxon (grayson 1984b, ch6)
do like presence absence but weigh signal according to abundance
and assume abundant species lives more toward center of tolerance 
grayson 1981: fossil accumulation affects datasets -> use with caution

taxon free 
cornelis van der klaauw 1948 (p 160) relation of traits of animals and livelihood = ecomorphology:
 eg what the animal eats also eg how diverse the place is. for environment mostly diet -> what plants grew and habitat -> climate etc
helps with not having to assume that closest relative tolerances were the same. (ch7)
eg usage of teeth: dental microwear. calandra and merceron 2016: analyze miniscratches on the surface to get diet to get vegetation and climate
    oma: to conduct this you need many samples of the same bone
    need to know exactly which bone it is since different teeth used for different things

diversity and size clines as enviromnental indicators
andrews et al 1979: how diverse also has a mapping to environment eg tropical is more diverse than arctic 
coarse indicators of the environment
lyman 2008: strong sample size effects: bigger sample is more diverse
so to do this you need equal size samples from all time periods analyzed
size clines basic idea: coarse indicators for size of bone -> environment eg larger libs warmer climate (mayr 1970)
also dental measures correlate with environment, eg mandibles (faith et al 2016). for that you need sufficient 
tooth samples and correct identification which tooth it is

end lesson: sample sizes and data precision are important. therefore it is superimportant 
to get more data / improve accuracy, which is the main goal of the digitization effort.
scale sense: faith lyman ch 4 said 1000 is solid 10 000 whopping sample size
dataset in question in this work has 90,000 so the value is pretty clear

\subsection{Composition of mammal teeth}

Fossils occur when animal / plant remains are deposited in a sediment in a way that preserves 
some part of its original form. Since teeth are the hardest material in animals, large fraction
of found parts are teeth. Fossil finding is followed by identification to most specific taxon possible
largely a technical skill (ch5), teeth are identified down to type and number, how manyeth the teeth are,
counting from center to edge or other way round??
specimen can be either one tooth or fragments of the jaw bone where there are multiple teeth (markings like M1-3)

from \cite{Hillson_2005} what teeth are composed of

 the jaw bones
lower jaw bones: mandibles
 permanent and deciduous (D), nonpermanent "milk" teeth (laita vaan jos löytyy d-hampaita)

right and left sides are always symmetrical, denoted simply L or R or Lt or Rt or left or right. left is left looking from the animal, not the observers perspective
Identicality also causes that sometimes tooth fossils are misidentified to the wrong side and corrected (ei lähteestä vaan nähty datasta koska l ja r on sutattu aika monta kertaa ja vaihettu

four classes, front to back: three incisors (I), one canine (C), four premolars (P), three molars (M). top bottom left right. top/bottom noting upper jaw as superscript lower jaw as lower script, 
 purpose: incisor -> catching, canine -> stabbing / killing prey, molars are for chewing. premolars are bit like canines bit like molars, function varies lot
 between taxa including holding, cutting and chewing. also form and number of each present changes between taxa.
sometimes lower jaw as line on top and upper jaw as line on bottom, sometimes both are used: upper script number with line on bottom. Line is "the other jaw"
if there are less of a type of teeth eg two premolars, they might be no 1 and 2 or no 3 and 4

% summary of chapter

\section{Experimental setup}

% introduction to chapter

\subsection{data description}

has been done by different annotators, no logs on who logged what, everyone 
had a bit different style of notating. also no clearly defined standard 
for notating specimens. so might be that actual data used will have 
characters or words not present in any data, causing errors.


Identicality also causes that sometimes tooth fossils are misidentified to 
the wrong side, seen in data with smudged over l's and r's

Catalogues have lines between entries. challenge for the model to not mark these as underlined
in these cases the line is long and spans the entire image, so it should be distinguishable from 
a single underlined character.

\subsubsection{Notes on creating the dataset}

Hand-labeled

Data was extracted from scans by getting bounding boxes from Azure Vision API,
finding the correct column (nature of specimen or element), and cropping the image 
according to bounding boxes.

Non-tooth samples were not discarded since they contain 
bone fossil related words and good samples of the handwriting style of this dataset.

smudged-over "L" was labeled as "R", and other way around: it seems that later 
someone found it was the opposite side after all. Hope of this is that the model 
would learn to map "messy L" as "R". snudged "left" or "right" was not noted as the 
opposite as there were too few such samples.

Superscript seems much more rare than lower script

Data was labeled not by individual characters but as full tooth descriptions
to preserve context where tooth special characters are more likely to occur

Some have been corrected by writing on top and thus are very hard 
even for humans to read, this is also an example of smudged-over correction: 

\includegraphics*[scale=0.2]{../images/superambiguous_data_sample.png}

\subsubsection{Unicode characters used for data labeling}

explain: unicode has graphenes with code points. eg a is one graphene one code point,
à is one graphene two code points (dot on top and the letter). the top thing -like characters will be called 
"modifiers".

markings contain letters and numbers with no line, line on top or line at the bottom.
Each character can be lower- or upper script. The modifiers used are: 
macron with lower ($\bar{\mathrm{A}}$) and upper variant.

Unicode \cite{unicode_homepage} has characters that are for example upper script, but 
these were not used for two reasons:

- lower and upper script character set is incomplete for this purpose (eg 3 with upper macron and lower script needed)

- from the model perspective 3 and $_3$ are no more similar than A and B, however, 
three combined with lower script modifier and 3 with upper script modifier 
all contain the same unicode character 3 with only the modifier changing. The 
problem here is that there is no lower or upper case modifiers in unicode. Therefore,
the caron ($\check{\mathrm{A}}$) was chosen as the lower script modifier, and the circumflex accent ($\hat{\mathrm{A}}$)
as upper script. These were chosen since the arrow-like modifier pointing up or down
is maybe the most logical placeholder for the missing modifier. More traditional 
workarounds of missing upper or lower script, the underscore "\_" and separate 
caret character "\^ " were not used to keep one unicode graphene represent one character 
on the page. Also on the other hand using one modifier for all lowercase characters allows 
the model to understand that there is a similarity between all lowercase characters.
The intention is that one idea about a character is encoded as one code point, so that 
the model can learn the mapping from the image of the character to the code point 
combination
(until now already in thesis text)
----

also: some annotators used / instead of line. left to / -> upper, right -> lower.
unknown/unsure noted with x with macrons on top/bottom
annotate with up/down macron

if model is toothornot classifier + tooth reader -> remove fractions notes

\subsection{Data preprocessing}

Convert to black and white since text reading should not change when color of writing / background changes?

Convert to background completely white, foreground completely black? Either there is a line or not?

\subsection{Methods: base models and transfer learning tehniques}

the sequence or character per character recognition question:
sequence is a mapping from image to variable-length phrase
character per character approach, inspired by \cite{tibetan_ocr}:
train a classifier: is this word a tooth or not? then give non-tooth to the untuned
trocr, which works very well. Then few shot transfer learn a classifier from an image
with only a tooth marking (letter and one number) to tooth. Possibly extend classifier to 
be able to recognize multiple teeth (eg M1-3). Target could be multivariate: first would
be tooth (which i1-3,c,p1-4,m1-3), second would be jaw (upper, lower, unknown), third side 
(left, right, unknown). Separating 'l/r/lt' from the letter+number tooth notation is 
fairly trivial: noncursive handwriting can be separated by finding a vertical line where there 
is no black. Image processing: convert to black and white, then find x coordinate with no black 
and split there.

sequence benefits: adaptable to many kinds and lengths of input, possible to get good inferences 
for surprising marking styles 
sequence bad sides: finetuning just one layer on 80 training images for two epochs took about 15 minutes
-> all hyperparameter optimization etc is out of question with this heavy training.
Also: not domain adaptation (adapting same task to different dataset), but task adaptation ie. 
the target set of characters has changed. Encoding this to the large encoder decoder transformers would 
require rewriting parts of the preprocessor and model which is too complex given the level of this work.

character by character benefits: feasible given available data and computing resources
possible to encode the classes (ie, teeth)
classifying characters has been essentially solved, easy problem 
also classifying to tooth or not tooth should be easy 
--> focus on model ensemble with tooth or not classifier + trocr + tooth classifier

\subsubsection{Encoding prior knowledge}
Priors. base model already knows the output should be a word, eg "jdaslkjflkds" is a highly unlikely
correct answer.
Bone notation has a very small subset of possible english words, eg. the word "beach ball" cannot ever be a correct 
answer for a reading

% summary of chapter

\section{Results and discussion}

% introduction to chapter

% summary of chapter

\section{Conclusions}

\printbibliography

\end{document}