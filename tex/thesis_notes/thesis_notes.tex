\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage[style=numeric,bibstyle=numeric,backend=biber,natbib=true,maxbibnames=99,giveninits=true,uniquename=init]{biblatex}
\usepackage[utf8]{inputenc}

\addbibresource{../bibliography.bib}
\title{Thesis notes}

\begin{document}

\tableofcontents

\section{Abstract}

Keywords: Optical character recognition, Few-shot transfer learning, Vision transformers, Paleontological databases

\section{Introduction}

% outline of an introduction
% introduce the broad research area, why this is interesting
% explanation of the specific problem
% brief review of standard solutions to this (or related) problem(s) and their limitation in this case (incl. reference key papers)
% outline of the new solution
% how the solution was evaluated, what were the results of this evaluations

% relevance for other work: why was this specific problem? how can this be concretely used?
relevance of this work: any field that does ocr on unconventional characters, or constrained vocabulary.
direct relevance to museums digitizing fossil data specifically, but could be any old handwriting.

% organization
The rest of this thesis is organized as follows.

\section{Background}

% introduction to chapter

\subsection{neural networks and deep learning}
keywords to explain (maybe) from \cite{li2021trocr}
- knowledge distillation
- generalist models
	- large unsupervised training data sets
- transformers
- cnns
- deep neural networks
- self attention
- convolution
- transfer learning
- encoder/decoder
- model compression
- loss functions
	- cross-entropy 
- data augmentation
- training/validation/test data sets
- learning rate
- batch size
- tokenizing
- image patches
- self-attention
- multi-head self-attention
- performance metrics
	- precision
	- recall 
	- f1

\subsection{Fundamentals on paleoecology}

This section will have a summary on what fossil data can be used for.

the why: why do this at all? why is accurate dental data relevant, in general?

\subsubsection{Basics on ecology}

Nature is highly complicated -> models, approximate laws and assumptions enable drawing conclusions from
known distributions of species. Assumptions hold usually, not always, in reality but need to be in place 
because nothing could be known without them

Tolerances and niches (fundamental + realized): basis for environmental reconstruction \cite{Faith_Lyman_2019} ch 2

Assume that nearest living relative has same tolerances now -> get past environment (ch3)

Presence/absence/abundance: (ch3) species living somewhere -> environment matches the fundamental niche. absent more complicated,
environment may or may not be suitable (eg a fossil sample was simply not found). abundance estimation harder, but more
fine-resolution environmental analysis

transfer function: paleobiotic data -> environment (ch2)

environment is constructed with a coarse resolution such as how many 10cm's of precipitation (ch2)

Theory that the data analysis relies on

\subsubsection{Paleoenvironmental reconstruction}

purpose of this section: how the data can be used

ch5 things

fundamental idea: you have a list of taxa occurring in a place at certain geological time.
how to map this list to an environment?

presence/absence approach: use information on what species were present, to lesser extent those absent (absence has many reasons so not as reliable)
either fix species or place. fix species: find where this set of species lives now (climate maps \& areas of sympatry), this indicates 
that historical locality had this climate. place-fixing: analyze how which species show up in this place
 changes over time, reduce species showing up data to lower dimension
(like pca), eg how many warm-climate vs cold-climate species show up, this gives ideas on changes in climate over time.

\subsubsection{Diets and evolution}

maybe, how the data is used

\subsubsection{Composition of mammal teeth}

Fossils occur when animal / plant remains are deposited in a sediment in a way that preserves 
some part of its original form. Since teeth are the hardest material in animals, large fraction
of found parts are teeth. Fossil finding is followed by identification to most specific taxon possible
largely a technical skill (ch5)

specimen can be either one tooth or fragments of the jaw bone where there are multiple teeth (markings like M1-3)

from \cite{Hillson_2005}

lower jaw bones: mandibles

right and left sides are always symmetrical, denoted simply L or R or Lt or Rt or left or right. left is left looking from the animal, not the observers perspective

Identicality also causes that sometimes tooth fossils are misidentified to the wrong side and corrected (ei lähteestä vaan nähty datasta koska l ja r on sutattu aika monta kertaa ja vaihettu

four classes, front to back: three incisors (I), one canine (C), four premolars (P), three molars (M). top bottom left right. top/bottom noting upper jaw as superscript lower jaw as lower script, 
sometimes lower jaw as line on top and upper jaw as line on bottom, sometimes both are used: upper script number with line on bottom. Line is "the other jaw"

deciduous (D), nonpermanent "milk" teeth (laita vaan jos löytyy d-hampaita)

if there are less of a type of teeth eg two premolars, they might be no 1 and 2 or no 3 and 4

give some example listing of all possible teeth in a mammal, both line and 
super sub script notation. give image for this

% summary of chapter

\section{Experimental setup}

% introduction to chapter

\subsection{data description}

has been done by different annotators, no logs on who logged what, everyone 
had a bit different style of notating. also no clearly defined standard 
for notating specimens. so might be that actual data used will have 
characters or words not present in any data, causing errors.


\subsubsection{Notes on creating the dataset}

Hand-labeled

Data was extracted from scans by getting bounding boxes from Azure Vision API,
finding the correct column (nature of specimen or element), and cropping the image 
according to bounding boxes.

Non-tooth samples were not discarded since they contain 
bone fossil related words and good samples of the handwriting style of this dataset.

smudged-over "L" was labeled as "R", and other way around: it seems that later 
someone found it was the opposite side after all. Hope of this is that the model 
would learn to map "messy L" as "R". snudged "left" or "right" was not noted as the 
opposite as there were too few such samples.

Superscript seems much more rare than lower script

Data was labeled not by individual characters but as full tooth descriptions
to preserve context where tooth special characters are more likely to occur

Some have been corrected by writing on top and thus are very hard 
even for humans to read, this is also an example of smudged-over correction: 

\includegraphics*[scale=0.2]{../images/superambiguous_data_sample.png}

\subsubsection{Unicode characters used for data labeling}

explain: unicode has graphenes with code points. eg a is one graphene one code point,
à is one graphene two code points (dot on top and the letter). the top thing -like characters will be called 
"modifiers".

markings contain letters and numbers with no line, line on top or line at the bottom.
Each character can be lower- or upper script. The modifiers used are: 
macron with lower ($\bar{\mathrm{A}}$) and upper variant.

Unicode \cite{unicode_homepage} has characters that are for example upper script, but 
these were not used for two reasons:

- lower and upper script character set is incomplete for this purpose (eg 3 with upper macron and lower script needed)

- from the model perspective 3 and $_3$ are no more similar than A and B, however, 
three combined with lower script modifier and 3 with upper script modifier 
all contain the same unicode character 3 with only the modifier changing. The 
problem here is that there is no lower or upper case modifiers in unicode. Therefore,
the caron ($\check{\mathrm{A}}$) was chosen as the lower script modifier, and the circumflex accent ($\hat{\mathrm{A}}$)
as upper script. These were chosen since the arrow-like modifier pointing up or down
is maybe the most logical placeholder for the missing modifier. More traditional 
workarounds of missing upper or lower script, the underscore "\_" and separate 
caret character "\^ " were not used to keep one unicode graphene represent one character 
on the page. Also on the other hand using one modifier for all lowercase characters allows 
the model to understand that there is a similarity between all lowercase characters.
The intention is that one idea about a character is encoded as one code point, so that 
the model can learn the mapping from the image of the character to the code point 
combination
(until now already in thesis text)
----

also: some annotators used / instead of line. left to / -> upper, right -> lower.
annotate with up/down macron

\subsection{Data preprocessing}

Convert to black and white since text reading should not change when color of writing / background changes?

Convert to background completely white, foreground completely black? Either there is a line or not?

\subsection{Methods: base models and transfer learning tehniques}

\subsubsection{Encoding prior knowledge}
Priors. base model already knows the output should be a word, eg "jdaslkjflkds" is a highly unlikely
correct answer.
Bone notation has a very small subset of possible english words, eg. the word "beach ball" cannot ever be a correct 
answer for a reading

% summary of chapter

\section{Results and discussion}

% introduction to chapter

% summary of chapter

\section{Conclusions}

\printbibliography

\end{document}